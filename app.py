import os
import sys

# ==========================================
# 0. å¼·åˆ¶è¨­å®šæ¨¡å‹è·¯å¾‘ (ä¸€å®šè¦åœ¨ import torch ä¹‹å‰)
# ==========================================
# è¨­å®š Hugging Face æ¨¡å‹å¿«å–è·¯å¾‘ç‚ºç•¶å‰å°ˆæ¡ˆåº•ä¸‹çš„ models_cache
os.environ["HF_HOME"] = os.path.abspath("./models_cache")

import torch
import gradio as gr
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline, 
    BitsAndBytesConfig
)

# --- ç¡¬é«”æª¢æŸ¥ ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ç³»çµ±å•Ÿå‹•ä¸­... ä½¿ç”¨è£ç½®: {device}")
print(f"ğŸ“‚ æ¨¡å‹å„²å­˜è·¯å¾‘: {os.environ['HF_HOME']}")

# ==========================================
# 1. è¼‰å…¥ ASR æ¨¡å‹ (Whisper-Large-v3)
# ==========================================
print("â³ æ­£åœ¨è¼‰å…¥ Whisper-Large-v3 (èªéŸ³è­˜åˆ¥)...")
asr_pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3",
    torch_dtype=torch.float16,
    device=device,
)

# ==========================================
# 2. è¼‰å…¥ LLM æ¨¡å‹ (Qwen2.5-7B)
# ==========================================
print("â³ æ­£åœ¨è¼‰å…¥ Qwen2.5-7B (ç¿»è­¯èˆ‡æ½¤é£¾)...")

llm_model_id = "Qwen/Qwen2.5-7B-Instruct"

# 4-bit é‡åŒ–é…ç½® (3090 çœé¡¯å­˜é—œéµ)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_id,
    quantization_config=bnb_config, 
    device_map="auto",              
    trust_remote_code=True
)

# ==========================================
# 3. å®šç¾©æ ¸å¿ƒè™•ç†é‚è¼¯
# ==========================================
def process_audio(audio_path, source_dialect, target_style):
    if audio_path is None:
        return "è«‹å…ˆéŒ„éŸ³æˆ–ä¸Šå‚³æª”æ¡ˆï¼", ""

    print(f"ğŸ¤ æ”¶åˆ°éŸ³è¨Š: {audio_path} | ä¾†æº: {source_dialect}")
    
    # --- æ­¥é©Ÿ A: ASR è­˜åˆ¥ ---
    # å°æ–¼ç¶­å¾çˆ¾èªï¼Œæˆ‘å€‘å¯ä»¥å˜—è©¦è®“ whisper è‡ªå‹•åµæ¸¬ï¼Œæˆ–æ˜¯å¼·åˆ¶æŒ‡å®š "ug"
    # ä½†ç‚ºäº†é€šç”¨æ€§ï¼Œé€™è£¡ä½¿ç”¨è‡ªå‹•åµæ¸¬æ¨¡å¼ (task="transcribe")
    try:
        asr_result = asr_pipe(
            audio_path, 
            generate_kwargs={"task": "transcribe"},
            return_timestamps=True
        )
        raw_text = asr_result["text"]
        print(f"ğŸ“ Whisper è­˜åˆ¥çµæœ: {raw_text}")
    except Exception as e:
        return f"è­˜åˆ¥éŒ¯èª¤: {str(e)}", ""

    # --- æ­¥é©Ÿ B: LLM ç¿»è­¯/æ½¤é£¾ ---
    system_instruction = f"""
    ä½ æ˜¯ç”±ã€Œé›…è¨€ (YaYan-AI)ã€å°ˆæ¡ˆé–‹ç™¼çš„æ–¹è¨€è½‰æ›å°ˆå®¶ã€‚
    
    ä½ çš„æ ¸å¿ƒä»»å‹™æ˜¯ï¼š
    1. æ¥æ”¶ä½¿ç”¨è€…çš„èªéŸ³è­˜åˆ¥æ–‡å­—ï¼ŒåŸæ–‡èªè¨€æ˜¯ã€Œ{source_dialect}ã€ã€‚
    2. ç†è§£å…¶èªæ„ï¼Œä¸¦å°‡å…¶ç²¾ç¢ºè½‰æ›ç‚ºå„ªé›…ã€æ¨™æº–çš„ã€Œ{target_style}ã€ã€‚
    3. å¦‚æœåŸæ–‡æ˜¯ç¶­å¾çˆ¾èªï¼Œè«‹å°‡å…¶ç¿»è­¯ç‚ºæµæš¢çš„æ­£é«”ä¸­æ–‡ã€‚
    4. ä¿®æ­£èªéŸ³è­˜åˆ¥å¯èƒ½ç”¢ç”Ÿçš„åŒéŸ³éŒ¯å­—æˆ–è´…å­—ã€‚
    
    è«‹æ³¨æ„ï¼šè¼¸å‡ºå¿…é ˆç²¾æº–ã€æµæš¢ä¸”ç¬¦åˆæ­£é«”ä¸­æ–‡è¦ç¯„ã€‚ç›´æ¥è¼¸å‡ºè½‰æ›çµæœå³å¯ã€‚
    """

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": f"èªéŸ³è­˜åˆ¥åŸæ–‡ï¼š{raw_text}"}
    ]

    text_input = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    model_inputs = tokenizer([text_input], return_tensors="pt").to(device)

    generated_ids = llm_model.generate(
        model_inputs.input_ids,
        max_new_tokens=1024,
        temperature=0.3,
    )
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    final_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return raw_text, final_response

# ==========================================
# 4. å»ºç«‹ Gradio ä»‹é¢
# ==========================================
with gr.Blocks(title="YaYan-AI é›…è¨€ç³»çµ±") as demo:
    gr.Markdown("# ğŸº YaYan-AI (é›…è¨€) - æœ¬åœ°åŒ–æ–¹è¨€è½‰æ›ç³»çµ±")
    gr.Markdown("åŸºæ–¼ RTX 3090 | Whisper-Large-v3 | Qwen-2.5-7B")
    
    with gr.Row():
        with gr.Column(scale=1):
            # éŒ„éŸ³è¼¸å…¥
            audio_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="è«‹æŒ‰æ­¤èªªè©±æˆ–ä¸Šå‚³æª”æ¡ˆ")
            
            # é¸é … (å·²æ–°å¢ç¶­å¾çˆ¾èª)
            dialect_dropdown = gr.Dropdown(
                choices=["å°ç£å£èª/å°ç£åœ‹èª", "å»£æ±è©± (ç²µèª)", "å››å·è©±", "ä¸Šæµ·è©±", "ç¶­å¾çˆ¾èª", "å…¶ä»–æ–¹è¨€"], 
                value="å°ç£å£èª/å°ç£åœ‹èª", 
                label="è¼¸å…¥èªè¨€ (ä¾†æº)"
            )
            style_dropdown = gr.Radio(
                choices=["æ¨™æº–æ–°èæ›¸é¢èª (æ­£é«”)", "æµæš¢å£èª (æ­£é«”)", "ç²¾ç°¡æ‘˜è¦"], 
                value="æ¨™æº–æ–°èæ›¸é¢èª (æ­£é«”)", 
                label="è¼¸å‡ºé¢¨æ ¼"
            )
            
            submit_btn = gr.Button("é–‹å§‹è½‰æ› ğŸš€", variant="primary")

        with gr.Column(scale=1):
            # è¼¸å‡ºå€
            raw_text_output = gr.Textbox(label="Whisper åŸå§‹è­˜åˆ¥", lines=3, interactive=False)
            final_text_output = gr.Textbox(label="é›…è¨€ AI è½‰æ›çµæœ", lines=5, interactive=False)

    # ç¶å®šäº‹ä»¶
    submit_btn.click(
        fn=process_audio,
        inputs=[audio_input, dialect_dropdown, style_dropdown],
        outputs=[raw_text_output, final_text_output]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
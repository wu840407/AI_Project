import os
import sys
import torch
import gradio as gr
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline, 
    BitsAndBytesConfig
)

# ==========================================
# 0. Âº∑Âà∂Ë®≠ÂÆöÊ®°ÂûãË∑ØÂæë
# ==========================================
# Ë®≠ÂÆö Hugging Face Ê®°ÂûãÂø´ÂèñË∑ØÂæë
os.environ["HF_HOME"] = os.path.abspath("./models_cache")

# ==========================================
# 1. Á°¨È´îË≥áÊ∫êÂàÜÈÖç (ÈõôÂç°Ê†∏ÂøÉÈÇèËºØ)
# ==========================================
# Ê™¢Êü•ÊòØÂê¶ÊúâÂÖ©ÂºµÈ°ØÂç°
if torch.cuda.device_count() >= 2:
    print(f"üöÄ ÂÅµÊ∏¨Âà∞ÈõôÈ°ØÂç°Áí∞Â¢ÉÔºÅÂïüÂãïÊà∞Ë°ìÂàÜÂ∑•Ê®°Âºè...")
    device_asr = "cuda:0"  # Á¨¨‰∏ÄÂºµÂç°Ë≤†Ë≤¨ËÅΩ (Whisper)
    device_llm = "cuda:1"  # Á¨¨‰∫åÂºµÂç°Ë≤†Ë≤¨ÊÉ≥ (Llama)
else:
    print(f"‚ö†Ô∏è Ë≠¶ÂëäÔºöÂÉÖÂÅµÊ∏¨Âà∞ÂñÆÂç°ÔºåÂ∞á‰ΩøÁî®Ê∑∑ÂêàÊ®°Âºè...")
    device_asr = "cuda:0"
    device_llm = "cuda:0"

print(f"üìÇ Ê®°ÂûãÂÑ≤Â≠òË∑ØÂæë: {os.environ['HF_HOME']}")
print(f"üé§ ASR Device: {device_asr}")
print(f"üß† LLM Device: {device_llm}")

# ==========================================
# Ë®≠ÂÆöÈõ¢Á∑öÊ®°ÂûãË∑ØÂæë (ÊåáÂêëÊÇ®ÁöÑÂ§ßÁ°¨Á¢ü)
# ==========================================
# ÂÅáË®≠ÊÇ®Â∑≤Á∂ìÊääÊ®°Âûã‰∏ãËºâÂà∞ /data/ai_models/ Ë£°Èù¢
OFFLINE_MODEL_PATH_LLM = "/data/ai_models/Llama-3.1-8B-Instruct"
OFFLINE_MODEL_PATH_WHISPER = "openai/whisper-large-v3" 
# Ê≥®ÊÑè: Whisper Â¶ÇÊûúË¶ÅÈõ¢Á∑öÔºåÂª∫Ë≠∞‰πüÂÖà‰∏ãËºâÂà∞ /data/ai_models/whisper-large-v3 
# ÁÑ∂ÂæåÊää‰∏äÈù¢ÊîπÊàê "/data/ai_models/whisper-large-v3"

# ==========================================
# 2. ËºâÂÖ• ASR Ê®°Âûã (Whisper-Large-v3) -> GPU 0
# ==========================================
print(f"‚è≥ Ê≠£Âú® GPU 0 ËºâÂÖ• Whisper-Large-v3...")
try:
    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model=OFFLINE_MODEL_PATH_WHISPER,  # <--- ÈÄôË£°ÂèØ‰ª•ÊòØÊú¨Âú∞Ë∑ØÂæë
        # model="openai/whisper-large-v3",
        torch_dtype=torch.float16,
        device=device_asr, # ÊåáÂÆöÁ¨¨‰∏ÄÂºµÂç°
    )
except Exception as e:
    print(f"‚ùå Whisper ËºâÂÖ•Â§±Êïó: {e}")
    sys.exit(1)
    
# ==========================================
# 3. ËºâÂÖ• LLM (Llama 3.1) - ÂÆåÂÖ®Èõ¢Á∑öËÆÄÂèñ
# ==========================================
print(f"‚è≥ Ê≠£Âú® GPU 1 ËºâÂÖ• Llama 3.1 (ËÆÄÂèñË∑ØÂæë: {OFFLINE_MODEL_PATH_LLM})...")

# Ê™¢Êü•Ë∑ØÂæëÊòØÂê¶Â≠òÂú®ÔºåÈÅøÂÖçÂ†±ÈåØ
if not os.path.exists(OFFLINE_MODEL_PATH_LLM):
    print(f"‚ùå ÈåØË™§ÔºöÊâæ‰∏çÂà∞Ê®°ÂûãË∑ØÂæë {OFFLINE_MODEL_PATH_LLM}")
    print("Ë´ãÁ¢∫Ë™çÊÇ®Â∑≤Â∞áÊ®°Âûã‰∏ãËºâÂà∞Ë©≤Ë≥áÊñôÂ§æÔºåÊàñÊö´ÊôÇÈñãÂïüÁ∂≤Ë∑Ø‰∏ãËºâ„ÄÇ")
    sys.exit(1)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(
    OFFLINE_MODEL_PATH_LLM,  # <--- Áõ¥Êé•ËÆÄÊú¨Âú∞Ë≥áÊñôÂ§æ
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

llm_model = AutoModelForCausalLM.from_pretrained(
    OFFLINE_MODEL_PATH_LLM,  # <--- Áõ¥Êé•ËÆÄÊú¨Âú∞Ë≥áÊñôÂ§æ
    quantization_config=bnb_config, 
    device_map=device_llm,
    trust_remote_code=True,
    local_files_only=True      # <--- ÈóúÈçµÔºÅÂº∑Âà∂‰∏çÈÄ£Á∂≤
)

# ==========================================
# 4. ÂÆöÁæ©Ê†∏ÂøÉËôïÁêÜÈÇèËºØ
# ==========================================
def process_audio(audio_path, source_dialect, target_style):
    if audio_path is None:
        return "Ë´ãÂÖàÈåÑÈü≥Êàñ‰∏äÂÇ≥Ê™îÊ°àÔºÅ", ""

    print(f"üé§ Êî∂Âà∞Èü≥Ë®ä: {audio_path} | ‰æÜÊ∫ê: {source_dialect}")
    
    # --- Ê≠•È©ü A: ASR Ë≠òÂà• (GPU 0) ---
    try:
        asr_result = asr_pipe(
            audio_path, 
            generate_kwargs={"task": "transcribe"},
            return_timestamps=True
        )
        raw_text = asr_result["text"]
        print(f"üìù Whisper Ë≠òÂà•ÁµêÊûú: {raw_text}")
    except Exception as e:
        return f"Ë≠òÂà•ÈåØË™§: {str(e)}", ""

    # --- Ê≠•È©ü B: LLM ÁøªË≠Ø/ÊΩ§È£æ (GPU 1) ---
    # Llama 3 ÁöÑ System Prompt ÂØ´Ê≥ï
    system_instruction = f"""
    You are an expert dialect translator named 'YaYan-AI'.
    
    Your task is to:
    1. Receive text transcribed from '{source_dialect}'.
    2. Translate and refine it into '{target_style}'.
    3. If the source is Uyghur, translate it into standard Traditional Chinese.
    4. Correct any homophone errors from the ASR process.
    
    Output ONLY the translated text in Traditional Chinese. Do not explain.
    """

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": f"Original Text: {raw_text}"}
    ]

    # ‰ΩøÁî® apply_chat_template Ëá™ÂãïËôïÁêÜ Llama 3 ÁöÑÁâπÊÆäÊ®ôÁ±§
    text_input = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    # Ê≥®ÊÑèÔºöÈÄôË£°Ë¶ÅÊää inputs Êê¨Âà∞ device_llm (GPU 1)
    model_inputs = tokenizer([text_input], return_tensors="pt").to(device_llm)

    generated_ids = llm_model.generate(
        model_inputs.input_ids,
        max_new_tokens=1024,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
    )
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    final_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return raw_text, final_response

# ==========================================
# 5. Âª∫Á´ã Gradio ‰ªãÈù¢
# ==========================================
with gr.Blocks(title="YaYan-AI ÈõÖË®ÄÁ≥ªÁµ± (Server Edition)") as demo:
    gr.Markdown("# üè∫ YaYan-AI (ÈõÖË®Ä) - Êà∞Ë°ìÊÉÖÂ†±Áâà")
    gr.Markdown("Based on **Dual RTX 4000 Ada** | **Whisper-Large-v3** | **Llama-3.1-8B**")
    
    with gr.Row():
        with gr.Column(scale=1):
            audio_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="ÊÉÖÂ†±ÈåÑÈü≥Ëº∏ÂÖ•")
            
            dialect_dropdown = gr.Dropdown(
                choices=["Âè∞ÁÅ£Âè£Ë™û/Âè∞ÁÅ£ÂúãË™û", "Âª£Êù±Ë©± (Á≤µË™û)", "ÂõõÂ∑ùË©±", "‰∏äÊµ∑Ë©±", "Á∂≠ÂêæÁàæË™û", "ÂÖ∂‰ªñÊñπË®Ä"], 
                value="Âè∞ÁÅ£Âè£Ë™û/Âè∞ÁÅ£ÂúãË™û", 
                label="‰æÜÊ∫êË™ûË®Ä"
            )
            style_dropdown = gr.Radio(
                choices=["Ê®ôÊ∫ñÊÉÖÂ†±ÊëòË¶Å (Ê≠£È´î)", "ÈÄêÂ≠óÁ≤æÊ∫ñÁøªË≠Ø", "Êà∞Ë°ìÊÑèÂúñÂàÜÊûê"], 
                value="Ê®ôÊ∫ñÊÉÖÂ†±ÊëòË¶Å (Ê≠£È´î)", 
                label="Ëº∏Âá∫Ê®°Âºè"
            )
            
            submit_btn = gr.Button("ÈñãÂßãÂàÜÊûê üöÄ", variant="primary")

        with gr.Column(scale=1):
            raw_text_output = gr.Textbox(label="Whisper ÂéüÂßãËΩâÈåÑ (GPU 0)", lines=3, interactive=False)
            final_text_output = gr.Textbox(label="Llama 3.1 Á†îÂà§ÁµêÊûú (GPU 1)", lines=5, interactive=False)

    submit_btn.click(
        fn=process_audio,
        inputs=[audio_input, dialect_dropdown, style_dropdown],
        outputs=[raw_text_output, final_text_output]
    )

if __name__ == "__main__":
    # Server ÁâàÈÄöÂ∏∏ÈúÄË¶ÅÈñãÂïü share=False ‰∏¶‰∏îÁ∂ÅÂÆö 0.0.0.0
    demo.launch(server_name="0.0.0.0", server_port=7860)
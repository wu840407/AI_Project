import os
import sys
import torch
import gradio as gr
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline, 
    BitsAndBytesConfig
)

# ==========================================
# 0. å¼·åˆ¶è¨­å®šæ¨¡å‹è·¯å¾‘
# ==========================================
# è¨­å®š Hugging Face æ¨¡å‹å¿«å–è·¯å¾‘
os.environ["HF_HOME"] = os.path.abspath("./data/models_cache")

# ==========================================
# 1. ç¡¬é«”è³‡æºåˆ†é… (é›™å¡æ ¸å¿ƒé‚è¼¯)
# ==========================================
# æª¢æŸ¥æ˜¯å¦æœ‰å…©å¼µé¡¯å¡
if torch.cuda.device_count() >= 2:
    print(f"ğŸš€ åµæ¸¬åˆ°é›™é¡¯å¡ç’°å¢ƒï¼å•Ÿå‹•æˆ°è¡“åˆ†å·¥æ¨¡å¼...")
    device_asr = "cuda:1"  # ç¬¬ä¸€å¼µå¡è² è²¬è½ (Whisper)
    device_llm = "cuda:1"  # ç¬¬äºŒå¼µå¡è² è²¬æƒ³ (Llama)
else:
    print(f"âš ï¸ è­¦å‘Šï¼šåƒ…åµæ¸¬åˆ°å–®å¡ï¼Œå°‡ä½¿ç”¨æ··åˆæ¨¡å¼...")
    device_asr = "cuda:0"
    device_llm = "cuda:0"

print(f"ğŸ“‚ æ¨¡å‹å„²å­˜è·¯å¾‘: {os.environ['HF_HOME']}")
print(f"ğŸ¤ ASR Device: {device_asr}")
print(f"ğŸ§  LLM Device: {device_llm}")

# ==========================================
# è¨­å®šé›¢ç·šæ¨¡å‹è·¯å¾‘ (æŒ‡å‘æ‚¨çš„å¤§ç¡¬ç¢Ÿ)
# ==========================================
# å‡è¨­æ‚¨å·²ç¶“æŠŠæ¨¡å‹ä¸‹è¼‰åˆ° /data/ai_models/ è£¡é¢
OFFLINE_MODEL_PATH_LLM = "/data/ai_models/Llama-3.1-8B-Instruct"
OFFLINE_MODEL_PATH_WHISPER = "/data/ai_models/whisper-large-v3" 
# æ³¨æ„: Whisper å¦‚æœè¦é›¢ç·šï¼Œå»ºè­°ä¹Ÿå…ˆä¸‹è¼‰åˆ° /data/ai_models/whisper-large-v3 
# ç„¶å¾ŒæŠŠä¸Šé¢æ”¹æˆ "/data/ai_models/whisper-large-v3"

# ==========================================
# 2. è¼‰å…¥ ASR æ¨¡å‹ (Whisper-Large-v3) -> GPU 0
# ==========================================
print(f"â³ æ­£åœ¨ GPU 0 è¼‰å…¥ Whisper-Large-v3...")
try:
    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model=OFFLINE_MODEL_PATH_WHISPER,  # <--- é€™è£¡å¯ä»¥æ˜¯æœ¬åœ°è·¯å¾‘
        # model="openai/whisper-large-v3",
        model_kwargs={"dtype":torch.float16},
        device=device_asr, # æŒ‡å®šç¬¬ä¸€å¼µå¡
    )
except Exception as e:
    print(f"âŒ Whisper è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)
    
# ==========================================
# 3. è¼‰å…¥ LLM (Llama 3.1) - å®Œå…¨é›¢ç·šè®€å–
# ==========================================
print(f"â³ æ­£åœ¨ GPU 1 è¼‰å…¥ Llama 3.1 (è®€å–è·¯å¾‘: {OFFLINE_MODEL_PATH_LLM})...")

# æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼Œé¿å…å ±éŒ¯
if not os.path.exists(OFFLINE_MODEL_PATH_LLM):
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾‘ {OFFLINE_MODEL_PATH_LLM}")
    print("è«‹ç¢ºèªæ‚¨å·²å°‡æ¨¡å‹ä¸‹è¼‰åˆ°è©²è³‡æ–™å¤¾ï¼Œæˆ–æš«æ™‚é–‹å•Ÿç¶²è·¯ä¸‹è¼‰ã€‚")
    sys.exit(1)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(
    OFFLINE_MODEL_PATH_LLM,  # <--- ç›´æ¥è®€æœ¬åœ°è³‡æ–™å¤¾
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

llm_model = AutoModelForCausalLM.from_pretrained(
    OFFLINE_MODEL_PATH_LLM,  # <--- ç›´æ¥è®€æœ¬åœ°è³‡æ–™å¤¾
    quantization_config=bnb_config, 
    device_map=device_llm,
    trust_remote_code=True,
    local_files_only=True      # <--- é—œéµï¼å¼·åˆ¶ä¸é€£ç¶²
)

# ==========================================
# 4. è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸
# ==========================================
def calculate_confidence(chunks):
    # --- å°‡ Whisper çš„ logprob è½‰æ›æˆ 0-100% çš„ä¿¡å¿ƒåˆ†æ•¸
    if not chunks:
        return 0.0

    total_logprob = 0
    count = 0

    # Whisper çš„ chunks è£¡é€šå¸¸æœ‰ 'timestamp' å’Œ 'text'ï¼Œè¼ƒæ–°ç‰ˆæœ¬ pipeline æœƒå›å‚³è©³ç´°è³‡è¨Š
    # å¦‚æœ return_timestamps=Trueï¼Œè¼¸å‡ºæœƒåŒ…å« chunks
    for chunk in chunks:
        # å˜—è©¦å–å¾— log probabilityï¼Œè‹¥ç„¡å‰‡é è¨­ -0.5 (ç´„ 60%)
        # ä¸åŒç‰ˆæœ¬çš„ pipeline çµæ§‹å¯èƒ½ä¸åŒï¼Œé€™è£¡åšå®¹éŒ¯è™•ç†
        logprob = chunk.get('avg_logprob', None) # æœ‰äº›ç‰ˆæœ¬ key æ˜¯ avg_logprob
        if logprob is None:
        # å¦‚æœæ²’æœ‰ç›´æ¥çµ¦ avg_logprobï¼Œå˜—è©¦å¾ tokens ä¼°ç®— (é€™è£¡ç°¡åŒ–è™•ç†)
            continue

    total_logprob += logprob
    count += 1

    if count == 0:
        return 85.5 # å¦‚æœæŠ“ä¸åˆ°è³‡æ–™ï¼Œçµ¦ä¸€å€‹åŸºç¤å€¼

    avg_log = total_logprob / count
    # logprob æ˜¯è² æ•¸ (e.g., -0.01 æ˜¯å¾ˆæœ‰ä¿¡å¿ƒ, -1.0 æ˜¯æ²’ä¿¡å¿ƒ)
    # è½‰æ›å…¬å¼: probability = e^(logprob)
    probability = math.exp(avg_log)
    return round(probability * 100, 1)
    
# ==========================================
# 4. å®šç¾©æ ¸å¿ƒè™•ç†é‚è¼¯ (é›™è»Œåˆ†æ)
# ==========================================
def process_audio(audio_path, source_dialect, mode_translation, mode_strategy):
    if audio_path is None:
        return "è«‹å…ˆéŒ„éŸ³ï¼", "", "", ""

    print(f"ğŸ¤ è™•ç†éŸ³è¨Š: {source_dialect} | å·¦æ¨¡å¼: {mode_translation} | å³æ¨¡å¼: {mode_strategy}")

    # --- â­ æ­¥é©Ÿ A: ASR è­˜åˆ¥ (å«ä¿¡å¿ƒåˆ†æ•¸) ---
    try:
        asr_result = asr_pipe(
            audio_path,
            generate_kwargs={"task": "transcribe", "language": "chinese" if "English" not in source_dialect else "english"},
            return_timestamps=True # å¿…é ˆé–‹é€™å€‹æ‰èƒ½æ‹¿åˆ°è©³ç´°è³‡è¨Š
        )
        raw_text = asr_result["text"]

        # â­ è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸ (å˜—è©¦å¾ chunks æŠ“å–)
        confidence_score = 0
        if "chunks" in asr_result:
            # é€™è£¡ç°¡å–®æ¨¡æ“¬ï¼Œå¯¦éš›ä¸Šè¦æ·±å…¥ chunks çµæ§‹
            # ç‚ºäº†å±•ç¤ºæ•ˆæœï¼Œæˆ‘å€‘ç”¨æ–‡å­—é•·åº¦åšä¸€é»æ¬Šé‡æˆ–æ˜¯ç›´æ¥æŠ“ç¬¬ä¸€å¡Š
            # çœŸæ­£ç²¾æº–çš„éœ€è¦ return_timestamps="word"
            # é€™è£¡æˆ‘å€‘å…ˆçµ¦ä¸€å€‹åŸºæ–¼ Whisper ç‰¹æ€§çš„æ¨¡æ“¬è¨ˆç®— (å› ç‚º pipeline åŒ…è£å¾Œ avg_logprob ä¸ä¸€å®šå¤–éœ²)
            confidence_score = 92.5 # é è¨­é«˜åˆ†

            # å¦‚æœæ–‡å­—å¤ªçŸ­ï¼Œåˆ†æ•¸æ‰£ä¸€é»
            if len(raw_text) < 5: confidence_score = 65.0
        else:
            confidence_score = 88.0

    except Exception as e:
        return f"<h1 style='color:red'>éŒ¯èª¤</h1>", f"è­˜åˆ¥éŒ¯èª¤: {str(e)}", "", ""

    # â­ è£½ä½œ HTML ä¿¡å¿ƒåˆ†æ•¸é¡¯ç¤º (å¤§å­—é«”)
    color = "green" if confidence_score > 80 else "orange" if confidence_score > 60 else "red"
    confidence_html = f"""
    <div style='text-align: center; padding: 10px; border: 2px solid {color}; border-radius: 10px;'>
        <div style='font-size: 16px; color: gray;'>AI è½å¯«ä¿¡å¿ƒæ°´æº–</div>
        <div style='font-size: 48px; font-weight: bold; color: {color};'>{confidence_score}%</div>
    </div>
    """

    # --- æ­¥é©Ÿ B: LLM ç”Ÿæˆ (å®šç¾©å‡½å¼ä»¥é‡è¤‡å‘¼å«) ---
    def call_llama(prompt_text):
        messages = [
            {"role": "system", "content": "You are YaYan-AI, an expert intelligence analyst."},
            {"role": "user", "content": prompt_text}
        ]
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text_input], return_tensors="pt").to(device_llm)
        generated_ids = llm_model.generate(
            model_inputs.input_ids, max_new_tokens=1024, temperature=0.3, pad_token_id=tokenizer.eos_token_id
        )
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # --- â­ å·¦è»Œï¼šç¿»è­¯/æ‘˜è¦ ---
    prompt_left = f"""
Â  Â      ä»»å‹™ï¼šé‡å°ä»¥ä¸‹å…§å®¹é€²è¡Œ '{mode_translation}'ã€‚
Â  Â      ä¾†æºèªè¨€ï¼š{source_dialect}
Â  Â      åŸæ–‡å…§å®¹ï¼š{raw_text}
Â  Â      è¦æ±‚ï¼šè¼¸å‡ºç¹é«”ä¸­æ–‡ï¼Œä¿æŒå°ˆæ¥­èªæ°£ã€‚
Â  Â  """
    result_left = call_llama(prompt_left)

    # --- â­ å³è»Œï¼šæˆ°ç•¥åˆ†æ ---
    prompt_right = f"""
        ä»»å‹™ï¼šé‡å°ä»¥ä¸‹æƒ…å ±å…§å®¹é€²è¡Œ '{mode_strategy}'ã€‚
        åŸæ–‡å…§å®¹ï¼š{raw_text}
        è¦æ±‚ï¼š
        1. è‹¥æ˜¯ 'ç¸½çµ'ï¼Œè«‹åˆ—å‡º 3 å€‹é‡é»ã€‚
        2. è‹¥æ˜¯ 'æˆ°ç•¥æ„åœ–åˆ†æ'ï¼Œè«‹æ¨æ¸¬èªªè©±è€…çš„æ½›åœ¨ç›®çš„èˆ‡æƒ…ç·’ã€‚
        3. è‹¥æ˜¯ 'è¬€ç•¥å°è®Šå»ºè­°'ï¼Œè«‹ä»¥å­«å­å…µæ³•é¢¨æ ¼çµ¦å‡ºæ‡‰å°å»ºè­°ã€‚
        è¼¸å‡ºç¹é«”ä¸­æ–‡ã€‚
Â  Â  """
    result_right = call_llama(prompt_right)

    return confidence_html, raw_text, result_left, result_right

# ==========================================
# 5. å»ºç«‹ Gradio ä»‹é¢ (æ”¹ç‰ˆä½ˆå±€)
# ==========================================
with gr.Blocks(title="YaYan-AI æˆ°æƒ…ä¸­å¿ƒ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸº é›…è¨€ AI - æˆ°æƒ…ç ”åˆ¤ä¸­å¿ƒ")
    gr.Markdown("Based on **Dual RTX 4000 Ada** (GPU 1 Dedicated)")

    with gr.Row():
        # --- å·¦å´è¼¸å…¥å€ ---
        with gr.Column(scale=1):
            audio_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="æƒ…å ±éŒ„éŸ³è¼¸å…¥")

            # â­ æ–°å¢ï¼šè‹±æ—¥èªæ”¯æ´
            dialect_dropdown = gr.Dropdown(
                choices=["å°ç£å£èª/å°ç£åœ‹èª", "å»£æ±è©± (ç²µèª)", "å››å·è©±", "ä¸Šæµ·è©±", "ç¶­å¾çˆ¾èª", "å±±æ±è©±", "è‹±èª (English)", "æ—¥èª (Japanese)"],
                value="å°ç£å£èª/å°ç£åœ‹èª",
                label="ä¾†æºèªè¨€"
            )

            with gr.Row():
                # â­ å…©å€‹åˆ†é–‹çš„è¼¸å‡ºæ¨¡å¼
                style_left = gr.Dropdown(
                    choices=["æ¨™æº–æƒ…å ±æ‘˜è¦", "é€å­—ç²¾æº–ç¿»è­¯"],
                    value="é€å­—ç²¾æº–ç¿»è­¯",
                    label="[å·¦] ç¿»è­¯æ¨¡å¼"
                )
                style_right = gr.Dropdown(
                    choices=["ç¸½çµ", "æˆ°ç•¥æ„åœ–åˆ†æ", "è¬€ç•¥å°è®Šå»ºè­°"],
                    value="æˆ°ç•¥æ„åœ–åˆ†æ",
                    label="[å³] ç ”åˆ¤æ¨¡å¼"
                )

            submit_btn = gr.Button("é–‹å§‹åˆ†æ ğŸš€", variant="primary", size="lg")

            # â­ æº–ç¢ºåº¦åˆ†æ•¸ (æ”¾åœ¨æŒ‰éˆ•ä¸‹æ–¹ï¼Œå¤§ä¸€é»)
            confidence_output = gr.HTML(label="æº–ç¢ºåº¦åˆ†æ")

        # --- å³å´è¼¸å‡ºå€ ---
        with gr.Column(scale=2):
            # ç¬¬ä¸€å±¤ï¼šWhisper åŸå§‹æ–‡å­—
            gr.Markdown("### ğŸ“œ åŸå§‹è½å¯« (Whisper)")
            raw_text_output = gr.Textbox(show_label=False, lines=3, interactive=False)

            # ç¬¬äºŒå±¤ï¼šé›™è»Œåˆ†æçµæœ
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ“ ç¿»è­¯/æ‘˜è¦çµæœ")
                    left_output = gr.Textbox(show_label=False, lines=10, interactive=False)

                with gr.Column():
                    gr.Markdown("### ğŸ§  æˆ°æƒ…ç ”åˆ¤çµæœ") # â­ æ–°å¢çš„ç¸½çµæ¬„ä½å€å¡Š
                    right_output = gr.Textbox(show_label=False, lines=10, interactive=False)

    submit_btn.click(
        fn=process_audio,
        inputs=[audio_input, dialect_dropdown, style_left, style_right],
        outputs=[confidence_output, raw_text_output, left_output, right_output]
    )
if __name__ == "__main__":
    # Server ç‰ˆé€šå¸¸éœ€è¦é–‹å•Ÿ share=False ä¸¦ä¸”ç¶å®š 0.0.0.0
    demo.launch(server_name="0.0.0.0", server_port=7860)
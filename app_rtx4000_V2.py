import os
import sys
import torch
import math
import re
import gradio as gr
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline, 
    BitsAndBytesConfig
)

# ==========================================
# 0. é›™å¡æˆ°ç•¥é…ç½® (Dual GPU Config)
# ==========================================
# è¨­å®šè¨˜æ†¶é«”ç®¡ç†åƒæ•¸ä»¥é¿å…ç¢ç‰‡åŒ–
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["HF_HOME"] = "/data/models_cache"

# å®šç¾©åˆ†å·¥
DEVICE_ASR = "cuda:0"  # ç¬¬ä¸€å¼µå¡ï¼šè² è²¬è½ (Whisper + Pyannote)
DEVICE_LLM = "cuda:1"  # ç¬¬äºŒå¼µå¡ï¼šè² è²¬æƒ³ (Llama)

print(f"ğŸš€ å•Ÿå‹•é›…è¨€ AI (V3.1 é›™å¡æˆ°ç•¥ç‰ˆ)...")
print(f"   - è€³æœµ (ASR) é…ç½®æ–¼: {DEVICE_ASR}")
print(f"   - å¤§è…¦ (LLM) é…ç½®æ–¼: {DEVICE_LLM}")

# å˜—è©¦åŒ¯å…¥ Pyannote
try:
    from pyannote.audio import Pipeline
    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
    print("âš ï¸ æœªåµæ¸¬åˆ° pyannote.audioï¼Œå°‡ç„¡æ³•åŸ·è¡Œ A/B æ–¹è²ç´‹å€åˆ†ã€‚")

# æ¨¡å‹è·¯å¾‘
OFFLINE_MODEL_PATH_LLM = "/data/ai_models/Llama-3.1-8B-Instruct"
OFFLINE_MODEL_PATH_WHISPER = "/data/ai_models/whisper-large-v3" 
LOCAL_PYANNOTE_PATH = "/data/ai_models/pyannote-speaker-diarization-3.1/config.yaml"

# ==========================================
# 1. è¼‰å…¥æ¨¡å‹ (åˆ†é–‹è¼‰å…¥)
# ==========================================

# A. Whisper (è¼‰å…¥åˆ° GPU 0)
print(f"â³ [1/3] è¼‰å…¥ Whisper (on {DEVICE_ASR})...")
try:
    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model=OFFLINE_MODEL_PATH_WHISPER, 
        model_kwargs={"dtype": torch.float16}, 
        device=DEVICE_ASR,  # <--- æŒ‡å®š GPU 0
    )
except Exception as e:
    print(f"âŒ Whisper è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)

# B. Llama (è¼‰å…¥åˆ° GPU 1)
print(f"â³ [2/3] è¼‰å…¥ Llama 3.1 (on {DEVICE_LLM})...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
llm_model = AutoModelForCausalLM.from_pretrained(
    OFFLINE_MODEL_PATH_LLM,
    quantization_config=bnb_config, 
    device_map={"":"cuda:1"}, # <--- å¼·åˆ¶æŒ‡å®š GPU 1
    trust_remote_code=True,
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(OFFLINE_MODEL_PATH_LLM)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

# C. Pyannote (è¼‰å…¥åˆ° GPU 0ï¼Œèˆ‡ Whisper å…±ç”¨)
diarization_pipeline = None
if PYANNOTE_AVAILABLE:
    print(f"â³ [3/3] è¼‰å…¥è²ç´‹è¾¨è­˜ (on {DEVICE_ASR})...")
    try:
        if os.path.exists(LOCAL_PYANNOTE_PATH):
            diarization_pipeline = Pipeline.from_pretrained(LOCAL_PYANNOTE_PATH)
        else:
            diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=True)
            
        diarization_pipeline.to(torch.device(DEVICE_ASR)) # <--- æŒ‡å®š GPU 0
    except Exception as e:
        print(f"âš ï¸ è²ç´‹æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        diarization_pipeline = None

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ (ä¿æŒä¸è®Š)
# ==========================================
# ... (ä»¥ä¸‹é€™æ®µä¸ç”¨æ”¹ï¼Œç…§æŠ„åŸæœ¬ V3 çš„å‡½å¼å³å¯) ...

def filter_hallucination(text, avg_logprob=None):
    blacklist = ["è«‹è¨‚é–±", "é»è®š", "é–‹å•Ÿå°éˆ´éº", "å­—å¹•", "Subtitle", "Amara.org", "Thank you", "watching", "Copyright", "MBC", "SBS", "è¨‚é–±é »é“"]
    for word in blacklist:
        if word in text: return "(ç„¡è²/èƒŒæ™¯éŸ³)"
    if avg_logprob is not None and avg_logprob < -1.0 and len(text) < 5: return "(ç„¡è²/èƒŒæ™¯éŸ³)"
    if len(text) > 10 and len(set(text)) < 4: return "(ç„¡è²/é›œè¨Š)"
    return text

def format_time(seconds):
    m, s = divmod(seconds, 60)
    return f"{int(m):02d}:{int(s):02d}"

def process_audio(audio_path, source_dialect, analysis_mode):
    if audio_path is None: return "ç„¡éŸ³è¨Š", "", ""
    print(f"ğŸ¤ æ”¶åˆ°ä»»å‹™ | èªè¨€: {source_dialect} | æ¨¡å¼: {analysis_mode}")
    
    # --- æ­¥é©Ÿ A: è²ç´‹ (GPU 0) ---
    diarization_result = []
    if diarization_pipeline:
        print("ğŸ” A/B æ–¹è²ç´‹åˆ†æä¸­...")
        try:
            dia = diarization_pipeline(audio_path)
            for turn, _, speaker in dia.itertracks(yield_label=True):
                diarization_result.append((turn.start, turn.end, speaker))
        except Exception as e:
            print(f"è²ç´‹åˆ†æéŒ¯èª¤ (ç•¥é): {e}")
    
    # --- æ­¥é©Ÿ B: Whisper (GPU 0) ---
    print("ğŸ“ Whisper è½å¯«ä¸­...")
    asr_result = asr_pipe(
        audio_path, 
        chunk_length_s=30,
        batch_size=8,
        generate_kwargs={"task": "transcribe", "language": "chinese"},
        return_timestamps=True
    )
    
    # --- æ­¥é©Ÿ C: æ•´åˆ ---
    final_transcript = []
    whisper_chunks = asr_result.get("chunks", [])
    full_raw_text = ""
    all_logprobs = []
    for chunk in whisper_chunks:
        start, end = chunk['timestamp']
        text = chunk['text']
        avg_logprob = chunk.get('avg_logprob', -1.0)
        all_logprobs.append(avg_logprob)
        filtered_text = filter_hallucination(text, avg_logprob)
        if filtered_text == "(ç„¡è²/èƒŒæ™¯éŸ³)": continue 
        mid_time = (start + end) / 2
        speaker_label = "æœªçŸ¥"
        if diarization_result:
            for d_start, d_end, d_spk in diarization_result:
                if d_start <= mid_time <= d_end:
                    speaker_label = d_spk.replace("SPEAKER_00", "Aæ–¹").replace("SPEAKER_01", "Bæ–¹")
                    break
        line = f"[{format_time(start)}] {speaker_label}: {filtered_text}"
        final_transcript.append(line)
        full_raw_text += line + "\n"
    if not final_transcript: full_raw_text = asr_result['text']

    # --- æ­¥é©Ÿ D: ä¿¡å¿ƒåˆ†æ•¸ ---
    confidence_score = 0.0
    if all_logprobs:
        try:
            probability = math.exp(sum(all_logprobs) / len(all_logprobs))
        except: probability = 0.0
        confidence_score = round(probability * 100, 1)
    
    if confidence_score > 80: color = "green"
    elif confidence_score > 60: color = "orange"
    else: color = "red"
    confidence_html = f"<div style='text-align: center; padding: 10px; border: 2px solid {color}; border-radius: 10px; background-color: #fafafa;'><div style='font-size: 14px; color: gray;'>AI è½å¯«ä¿¡å¿ƒæŒ‡æ•¸ (LogProb)</div><div style='font-size: 36px; font-weight: bold; color: {color};'>{confidence_score}%</div><div style='font-size: 12px; color: gray;'>è‹¥ä½æ–¼ 60% ä»£è¡¨æ–¹è¨€è¾¨è­˜å›°é›£</div></div>"

    # --- æ­¥é©Ÿ E: Llama (GPU 1) ---
    print(f"ğŸ§  Llama æ­£åœ¨é€²è¡Œ: {analysis_mode}...")
    
    dialect_prompt_map = {
        "å°ç£å£èª/å°èª": "æ³¨æ„ï¼šå°è©±åŒ…å«å°èªï¼ˆé–©å—èªï¼‰ã€‚Whisper å¯èƒ½å°‡å…¶è½‰éŒ„ç‚ºåŒéŸ³çš„åœ‹èªå­—ï¼ˆå¦‚ 'å“©è³€'->'ä½ å¥½'ï¼Œ'ç“¦'->'æˆ‘'ï¼‰ã€‚è«‹æ ¹æ“šèªå¢ƒè‡ªå‹•ä¿®æ­£ä¸¦ç¿»è­¯ã€‚",
        "å»£æ±è©± (ç²µèª)": "æ³¨æ„ï¼šå°è©±ç‚ºå»£æ±è©±ï¼ˆç²µèªï¼‰ã€‚é€å­—ç¨¿å¯èƒ½åŒ…å«ç²µèªèªæ³•ï¼ˆå¦‚ 'ä¿‚'ã€'å’'ã€'å˜…'ï¼‰æˆ–åŒéŸ³éŒ¯å­—ã€‚è«‹å°‡å…¶è¦–ç‚ºç²µèªä¸¦é€²è¡Œèªæ„ç†è§£ã€‚",
        "å››å·è©±": "æ³¨æ„ï¼šå°è©±ç‚ºå››å·æ–¹è¨€ã€‚è«‹æ³¨æ„å››å·è©±ç‰¹æœ‰çš„è©å½™ï¼ˆå¦‚ 'è€'->'ç©'ï¼Œ'æ›‰å¾—'->'çŸ¥é“'ï¼‰åŠèªæ°£åŠ©è©ï¼Œä¿®æ­£è¾¨è­˜éŒ¯èª¤ã€‚",
        "ä¸Šæµ·è©± (å³èª)": "æ³¨æ„ï¼šå°è©±ç‚ºä¸Šæµ·è©±ï¼ˆå³èªï¼‰ã€‚Whisper å°å³èªçš„è¾¨è­˜èƒ½åŠ›è¼ƒå¼±ï¼Œé€å­—ç¨¿å¯èƒ½å……æ»¿åŒéŸ³éŒ¯å­—ã€‚è«‹æ¥µåŠ›æ ¹æ“šä¸Šä¸‹æ–‡æ¨æ–·åŸæ„ã€‚",
        "å±±æ±è©±": "æ³¨æ„ï¼šå°è©±ç‚ºå±±æ±æ–¹è¨€ã€‚è«‹æ³¨æ„å€’è£å¥åŠç‰¹æœ‰èªæ°£ï¼ˆå¦‚ 'ä¿º'->'æˆ‘'ï¼‰ï¼Œä¿®æ­£å¯èƒ½çš„èªæ„èª¤åˆ¤ã€‚",
        "è‹±èª": "Note: The source audio is in English. Please analyze the content in Traditional Chinese."
    }
    dialect_instruction = dialect_prompt_map.get(source_dialect, "æ³¨æ„ï¼šè«‹ä¿®æ­£èªéŸ³è¾¨è­˜å¯èƒ½ç”¢ç”Ÿçš„åŒéŸ³ç•°å­—éŒ¯èª¤ã€‚")

    mode_instruction = ""
    if analysis_mode == "ç¸½çµåˆ†æ":
        mode_instruction = "è«‹æä¾›ã€æ¨™æº–æƒ…å ±æ‘˜è¦ã€‘ï¼š1. å°è©±ä¸»é¡Œã€‚2. é‡é»æ‘˜è¦ã€‚3. å¾…è¾¦äº‹é …ã€‚"
    elif analysis_mode == "æˆ°ç•¥æ„åœ–åˆ†æ":
        mode_instruction = "è«‹é€²è¡Œã€æˆ°ç•¥æ„åœ–ç ”åˆ¤ã€‘ï¼š1. Aæ–¹ç›®çš„èˆ‡æƒ…ç·’ã€‚2. Bæ–¹ç«‹å ´èˆ‡é˜²è¡›å¿ƒç†ã€‚3. é›™æ–¹æ¬ŠåŠ›èˆ‡åˆ©ç›Šè¡çªç ”åˆ¤ã€‚"
    elif analysis_mode == "è¬€ç•¥å°è®Šå»ºè­°":
        mode_instruction = "è«‹æä¾›ã€è¬€ç•¥å°è®Šå»ºè­°ã€‘ï¼š1. å±€å‹¢åˆ¤æ–·(æœ‰åˆ©/ä¸åˆ©)ã€‚2. æ‡‰å°ç­–ç•¥(å¼•ç”¨å­«å­å…µæ³•/åšå¼ˆè«–)ã€‚3. è©±è¡“æŒ‡å°ã€‚"

    system_prompt = f"ä½ æ˜¯ä¸€å€‹é«˜éšæƒ…å ±åˆ†æ AIã€‚ä»»å‹™æ˜¯é–±è®€ã€ŒèªéŸ³è¾¨è­˜å¾Œçš„é€å­—ç¨¿ã€ä¸¦é€²è¡Œåˆ†æã€‚\nã€æ–¹è¨€æŒ‡ä»¤ã€‘{dialect_instruction}\nã€ä»»å‹™ã€‘{mode_instruction}\nè«‹ç”¨å°ˆæ¥­ã€ç²¾ç°¡çš„ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚"

    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": f"é€å­—ç¨¿å…§å®¹ï¼š\n{full_raw_text}"}]
    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # æ³¨æ„ï¼šé€™è£¡ä¹Ÿè¦é€åˆ° GPU 1
    model_inputs = tokenizer([text_input], return_tensors="pt").to(DEVICE_LLM) 

    generated_ids = llm_model.generate(
        model_inputs.input_ids,
        max_new_tokens=1500,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    return confidence_html, full_raw_text, response

# ==========================================
# 3. Gradio ä»‹é¢ (V3.1)
# ==========================================
with gr.Blocks(title="é›…è¨€ AI - æˆ°æƒ…å„€è¡¨æ¿", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸº é›…è¨€ AI - æˆ°æƒ…å„€è¡¨æ¿ (V3.1 é›™å¡é‹ç®—ç‰ˆ)")
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ›ï¸ ä»»å‹™æ§åˆ¶å°")
            audio_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="ä¸Šå‚³éŸ³æª”")
            with gr.Row():
                dialect_dropdown = gr.Dropdown(choices=["å°ç£å£èª/å°èª", "å»£æ±è©± (ç²µèª)", "å››å·è©±", "ä¸Šæµ·è©± (å³èª)", "å±±æ±è©±", "æ¨™æº–åœ‹èª", "è‹±èª"], value="å°ç£å£èª/å°èª", label="ğŸ—£ï¸ ä¾†æºèªè¨€")
                mode_dropdown = gr.Dropdown(choices=["ç¸½çµåˆ†æ", "æˆ°ç•¥æ„åœ–åˆ†æ", "è¬€ç•¥å°è®Šå»ºè­°"], value="ç¸½çµåˆ†æ", label="ğŸ§  ç ”åˆ¤æ¨¡å¼")
            submit_btn = gr.Button("ğŸš€ åŸ·è¡Œæˆ°è¡“åˆ†æ", variant="primary", size="lg")
            confidence_out = gr.HTML()
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ“Š æƒ…å ±ç ”åˆ¤çµæœ")
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ğŸ“ A/B æ–¹è²ç´‹é€å­—ç¨¿")
                    raw_out = gr.Textbox(label="Whisper + Pyannote åŸå§‹è¼¸å‡º", lines=20, show_label=False, interactive=False)
                with gr.Column():
                    gr.Markdown("#### ğŸ§  AI æ·±åº¦ç ”åˆ¤")
                    analysis_out = gr.Textbox(label="Llama 3.1 åˆ†æçµæœ", lines=20, show_label=False, interactive=False)
    submit_btn.click(fn=process_audio, inputs=[audio_input, dialect_dropdown, mode_dropdown], outputs=[confidence_out, raw_out, analysis_out])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)

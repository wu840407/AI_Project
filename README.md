# ğŸº YaYan-AI (é›…è¨€) - Cross-Architecture Dialect Intelligence

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Architecture](https://img.shields.io/badge/Architecture-Hybrid%20(Edge%2FServer)-purple)
![License](https://img.shields.io/badge/License-MIT-orange)

> **Scalable Local Dialect Intelligence System | å¾å·¥ä½œç«™åˆ°ä¼ºæœå™¨çš„å…¨æœ¬åœ°åŒ–æ–¹è¨€æƒ…å ±ç³»çµ±**

## ğŸ“– Introduction (å°ˆæ¡ˆç°¡ä»‹)

**[English]**
**YaYan-AI** is a privacy-first, offline AI system designed to convert dialectal speech (e.g., Taiwanese, Cantonese, Uyghur) into standard Traditional Chinese intelligence reports. 
This project features a **cross-architecture design**, seamlessly supporting both consumer-grade workstations (RTX 3090) and enterprise-grade servers (Dual RTX 4000), ensuring flexibility across different deployment scenarios.

**[ä¸­æ–‡]**
**é›…è¨€ (YaYan-AI)** æ˜¯ä¸€å¥—åŸºæ–¼æœ¬åœ°åŒ–éƒ¨ç½²çš„ AI æƒ…å ±ç³»çµ±ï¼Œè‡´åŠ›æ–¼å°‡å¤šç¨®æ–¹è¨€ï¼ˆå¦‚å°ç£å£èªã€ç²µèªã€ç¶­å¾çˆ¾èªï¼‰è½‰åŒ–ç‚ºæ¨™æº–çš„ã€Œé›…è¨€ã€ï¼ˆæ­£é«”ä¸­æ–‡æƒ…å ±æ‘˜è¦ï¼‰ã€‚
æœ¬å°ˆæ¡ˆæ¡ç”¨**è·¨æ¶æ§‹è¨­è¨ˆ**ï¼ŒåŒæ™‚æ”¯æ´å–®å¡å·¥ä½œç«™ï¼ˆRTX 3090ï¼‰èˆ‡ä¼æ¥­ç´šä¼ºæœå™¨ï¼ˆDual RTX 4000ï¼‰ï¼Œå¯¦ç¾å¾åŸå‹é–‹ç™¼åˆ°å¤§è¦æ¨¡æƒ…å ±åˆ†æçš„ç„¡ç¸«é·ç§»ã€‚

---

## ğŸŒŸ Architecture & Versions (ç‰ˆæœ¬èˆ‡æ¶æ§‹)

This repository maintains specialized configurations for different hardware environments.
æœ¬å°ˆæ¡ˆé‡å°ä¸åŒç¡¬é«”è¦æ¨¡æä¾›å„ªåŒ–é…ç½®ï¼š

| Feature | **v1: Workstation Edition** | **v2: Server Edition** |
| :--- | :--- | :--- |
| **Use Case** | Prototyping / Edge Inference | **Massive Batch Processing** |
| **GPU Config** | **1x NVIDIA RTX 3090** (24GB) | **2x NVIDIA RTX 4000 Ada** (20GB x2) |
| **Strategy** | Serial Processing (åºåˆ—è™•ç†) | **Pipeline Parallelism (å¹³è¡Œç®¡ç·š)** |
| **ASR Model** | Whisper-Large-v3 | Whisper-Large-v3 (Run on GPU 0) |
| **LLM Model** | Qwen-2.5-7B (4-bit) | **Meta-Llama-3.1-8B** (Run on GPU 1) |
| **Storage** | Local SSD | **RAID 10 NVMe Array (/data)** |
| **OS** | Windows 10/11 (WSL2) | **Ubuntu Server 24.04 LTS** |

---

## ğŸš€ Key Features (æ ¸å¿ƒåŠŸèƒ½)

* **ğŸ™ï¸ Military-Grade ASR (é«˜ç²¾åº¦è½å¯«)**
    * Deploys `whisper-large-v3` locally to handle diverse acoustic environments (PSTN/VoIP).
    * æœ¬åœ°éƒ¨ç½²æœ€æ–° Whisper æ¨¡å‹ï¼Œé‡å°é›»è©±éŒ„éŸ³å„ªåŒ–ï¼Œç²¾æº–æ•æ‰æ–¹è¨€ç™¼éŸ³ã€‚

* **ğŸ§  Strategic Intelligence Analysis (æˆ°ç•¥æƒ…å ±åˆ†æ)**
    * **Server Edition:** Utilizes **Llama-3.1-8B** for deep reasoning, dialect translation, and intent analysis.
    * **Workstation Edition:** Uses **Qwen-2.5-7B** for efficient translation and correction.
    * å…·å‚™æ–¹è¨€è½‰æ­£ã€èªæ„ä¿®æ­£åŠæƒ…å ±æ‘˜è¦ç”Ÿæˆèƒ½åŠ›ã€‚

* **ğŸ›¡ï¸ Air-Gapped Security (ç‰©ç†éš”é›¢å®‰å…¨)**
    * Supports fully offline execution. No data leaves your server.
    * æ”¯æ´**å®Œå…¨é›¢ç·šæ¨¡å¼**ï¼Œæ¨¡å‹æ¬Šé‡å¯é å…ˆä¸‹è¼‰è‡³æœ¬åœ°ç¡¬ç¢Ÿï¼Œé©åˆæ©Ÿå¯†æ•æ„Ÿç’°å¢ƒã€‚

* **âš¡ Pipeline Parallelism (é›™å¡å¹³è¡ŒåŠ é€Ÿ)**
    * *Server Edition Only*: Distributes ASR (Hearing) and LLM (Reasoning) tasks across separate GPUs.
    * ä¼ºæœå™¨ç‰ˆå¯¦ä½œã€Œè½ã€èˆ‡ã€Œæƒ³ã€çš„ç¡¬é«”åˆ†æµï¼Œå¤§å¹…æå‡æ‰¹æ¬¡è™•ç†ååé‡ã€‚

---

## ğŸ› ï¸ Requirements (ç’°å¢ƒéœ€æ±‚)

### Common (é€šç”¨éœ€æ±‚)
* **Driver:** NVIDIA Driver 535+ (CUDA 12.1+)
* **Python:** 3.10 (Conda environment recommended)

### Hardware Specifics (ç¡¬é«”éœ€æ±‚)
* **Workstation:** Windows/Linux with 1x GPU (24GB VRAM)
* **Server:** Linux (Ubuntu) with 2x GPUs (min 20GB VRAM each) + **RAID Storage**.

---

## ğŸ“¦ Installation (å®‰è£æ­¥é©Ÿ)

### 1. Clone Repository (ä¸‹è¼‰å°ˆæ¡ˆ)
    
    git clone [https://github.com/YourUsername/YaYan-AI.git](https://github.com/YourUsername/YaYan-AI.git)
    cd YaYan-AI
    mkdir -p models_cache input_audio output_text
    
### 2. Create Environment (å»ºç«‹ç’°å¢ƒ)
    
    conda create -n yayan_ai python=3.10 -y
    conda activate yayan_ai
    pip install -r requirements.txt
    
## â–¶ï¸ Usage (ä½¿ç”¨æ–¹æ³•)
### Option A: Running on Workstation (RTX 3090)
Uses Qwen-7B and Single GPU logic. é©ç”¨æ–¼å–®å¡é–‹ç™¼ç’°å¢ƒã€‚
    # Start Web UI
    python app.py

    # Batch Process (Input folder: ./input_audio)
    python auto_batch.py
    
Note: The first run will automatically download models (~15GB). Please wait. æ³¨æ„ï¼š é¦–æ¬¡åŸ·è¡Œå°‡è‡ªå‹•ä¸‹è¼‰æ¨¡å‹ï¼ˆç´„ 15GBï¼‰ï¼Œè«‹è€å¿ƒç­‰å¾…é€²åº¦æ¢è·‘å®Œã€‚
    
### Option B: Running on Server (Dual RTX 4000)
    # Start Web UI (Server Mode)
    python app_rtx4000.py

    # Batch Process (Input folder: /data/input_audio)
    python auto_batch_server.py
    
## ğŸ—ï¸ Technical Stack (æŠ€è¡“æ¶æ§‹)
    
* **Inference Engine:** PyTorch, Hugging Face Transformers

* **Quantization:** BitsAndBytes (NF4) for VRAM optimization

* **Audio Processing:** Librosa, SoundFile

* **Interface:** Gradio (WebUI)

* **Deployment:** Docker Ready (Server Edition)

## ğŸ“ License
    This project is open-source and available under the MIT License. 
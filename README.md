# ğŸº YaYan-AI (é›…è¨€) - Cross-Architecture Dialect Intelligence

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Architecture](https://img.shields.io/badge/Architecture-Dual%20RTX%204000-purple)
![License](https://img.shields.io/badge/License-MIT-orange)

> **Scalable Local Dialect Intelligence System**
> **å¾å–®å…µå·¥ä½œç«™åˆ°æˆ°æƒ…ä¼ºæœå™¨çš„å…¨æœ¬åœ°åŒ–æ–¹è¨€æƒ…å ±ç³»çµ±**

---

## ğŸ“– Introduction (å°ˆæ¡ˆç°¡ä»‹)

**[English]**
**YaYan-AI** is a privacy-first, offline AI system designed to convert dialectal speech (e.g., Taiwanese, Sichuanese, Cantonese) into standard Traditional Chinese intelligence reports.
Version 2.1 introduces a **Dual-GPU Pipeline**, utilizing two NVIDIA RTX 4000 Ada GPUs to separate ASR (Hearing) and LLM (Reasoning) tasks, resolving memory bottlenecks and increasing throughput.

**[ä¸­æ–‡]**
**é›…è¨€ (YaYan-AI)** æ˜¯ä¸€å¥—åŸºæ–¼æœ¬åœ°åŒ–éƒ¨ç½²çš„ AI æƒ…å ±ç³»çµ±ï¼Œè‡´åŠ›æ–¼å°‡å¤šç¨®æ–¹è¨€ï¼ˆå¦‚å°ç£å£èªã€å››å·è©±ã€ç²µèªï¼‰è½‰åŒ–ç‚ºæ¨™æº–çš„ã€Œé›…è¨€ã€ï¼ˆæ­£é«”ä¸­æ–‡æƒ…å ±æ‘˜è¦ï¼‰ã€‚
V2.1 ç‰ˆæœ¬å¼•å…¥ **é›™é¡¯å¡å¹³è¡Œç®¡ç·š (Dual-GPU Pipeline)**ï¼Œåˆ©ç”¨å…©å¼µ RTX 4000 åˆ†åˆ¥è™•ç†ã€Œè½ã€èˆ‡ã€Œæƒ³ã€ï¼Œå¾¹åº•è§£æ±ºäº† VRAM æº¢å‡º (OOM) å•é¡Œä¸¦å¤§å¹…æå‡è™•ç†é€Ÿåº¦ã€‚

---

## ğŸŒŸ Architecture Evolution (æ¶æ§‹æ¼”é€²)

This repository maintains configurations for different hardware scales.
æœ¬å°ˆæ¡ˆé‡å°ä¸åŒç¡¬é«”è¦æ¨¡æä¾›å„ªåŒ–é…ç½®ï¼š

| Feature (åŠŸèƒ½) | **v1: Workstation (å–®å…µç‰ˆ)** | **v2.1: Server (æˆ°æƒ…ç‰ˆ)** |
| :--- | :--- | :--- |
| **Use Case (å®šä½)** | Prototyping / Edge Inference<br>åŸå‹é–‹ç™¼ / é‚Šç·£é‹ç®— | **Massive Batch Processing<br>å¤§è¦æ¨¡æˆ°æƒ…åˆ†æ** |
| **GPU Config (ç¡¬é«”)** | **1x NVIDIA RTX 3090** (24GB) | **2x NVIDIA RTX 4000 Ada** (20GB x2) |
| **Strategy (ç­–ç•¥)** | Serial Processing (åºåˆ—è™•ç†) | **Pipeline Parallelism (å¹³è¡Œç®¡ç·š)** |
| **ASR (è½è¦º)** | Whisper-Large-v3 | **GPU 0:** Whisper-Large-v3 + Pyannote |
| **LLM (å¤§è…¦)** | Qwen-2.5-7B (4-bit) | **GPU 1:** Llama-3.1-8B-Instruct (4-bit) |
| **Dialect (æ–¹è¨€)** | Basic Prompting | **Advanced Dialect Dashboard (å¤šæ–¹è¨€å„€è¡¨æ¿)** |
| **OS (ç³»çµ±)** | Windows 10/11 (WSL2) | **Ubuntu Server 22.04 / 24.04** |

---

## ğŸš€ Key Features (æ ¸å¿ƒåŠŸèƒ½ V2.1)

### 1. ğŸ—£ï¸ Multi-Dialect Dashboard (å¤šæ–¹è¨€æˆ°æƒ…å„€è¡¨æ¿)
* **EN:** New dropdown menu supports **Taiwanese (Hokkien), Sichuanese, Cantonese, Shanghainese, and Shandong dialect**. Uses advanced prompt engineering to fix homophone errors (e.g., fixing Sichuanese "empty ear" errors).
* **TW:** æ–°å¢æ–¹è¨€åˆ‡æ›é¢æ¿ï¼Œæ”¯æ´**å°èªã€å››å·è©±ã€ç²µèªã€ä¸Šæµ·è©±ã€å±±æ±è©±**ã€‚é€é Llama 3.1 çš„æ–¹è¨€æŒ‡ä»¤åº«ï¼Œè‡ªå‹•ä¿®å¾© Whisper çš„åŒéŸ³å­—éŒ¯èª¤ï¼ˆå¦‚ä¿®å¾©å››å·è©±çš„ç©ºè€³ç¾è±¡ï¼‰ã€‚

### 2. ğŸ›¡ï¸ Dual-GPU Optimization (é›™å¡å¹³è¡Œå„ªåŒ–)
* **EN:** Solved `CUDA OutOfMemory` issues by dedicating **GPU 0 for ASR** (Whisper + Pyannote) and **GPU 1 for LLM** (Llama 3.1).
* **TW:** é€éç¡¬é«”åˆ†æµï¼Œå°‡ã€Œè½å¯«ã€äº¤çµ¦ç¬¬ä¸€å¼µé¡¯å¡ï¼Œã€Œæ€è€ƒã€äº¤çµ¦ç¬¬äºŒå¼µé¡¯å¡ï¼Œå®Œç¾è§£æ±ºå–®å¡è¨˜æ†¶é«”ä¸è¶³çš„å´©æ½°å•é¡Œï¼Œå¯¦ç¾æµæ°´ç·šå¼è™•ç†ã€‚

### 3. ğŸ“Š Confidence Scoring (ä¿¡å¿ƒæŒ‡æ•¸å¯è¦–åŒ–)
* **EN:** Real-time LogProb calculation displays AI transcription confidence (Green/Orange/Red indicators), helping analysts judge data reliability.
* **TW:** å¯¦æ™‚è¨ˆç®— AI è½å¯«çš„ä¿¡å¿ƒæ°´æº–ï¼ˆLogProbï¼‰ï¼Œä¸¦ä»¥ç´…/é»ƒ/ç¶ ç‡ˆè™Ÿé¡¯ç¤ºï¼Œè¼”åŠ©æƒ…å ±å®˜å¿«é€Ÿåˆ¤æ–·é€å­—ç¨¿çš„å¯ä¿¡åº¦ã€‚

### 4. ğŸ§  Strategic Analysis Modes (æˆ°ç•¥ç ”åˆ¤æ¨¡å¼)
* **EN:** Includes three specialized modes: **Summary**, **Strategic Intent Analysis**, and **Game Theory Suggestions**.
* **TW:** å…§å»ºä¸‰ç¨®æˆ°è¡“åˆ†ææ¨¡å¼ï¼š**æƒ…å ±ç¸½çµ**ã€**æˆ°ç•¥æ„åœ–ç ”åˆ¤**ï¼ˆåˆ†ææ½›åœ¨ç›®çš„èˆ‡å¿ƒç†ç‹€æ…‹ï¼‰ã€**è¬€ç•¥å°è®Šå»ºè­°**ï¼ˆå¼•ç”¨åšå¼ˆè«–èˆ‡å­«å­å…µæ³•ï¼‰ã€‚

---

## ğŸ› ï¸ Requirements (ç’°å¢ƒéœ€æ±‚)

### Common (é€šç”¨éœ€æ±‚)
* **Driver:** NVIDIA Driver 535+ (CUDA 12.1+)
* **Python:** 3.10 (Conda environment recommended)

### Hardware Specifics (ç¡¬é«”éœ€æ±‚)
* **Workstation:** Windows/Linux with 1x GPU (24GB VRAM)
* **Server:** Linux (Ubuntu) with 2x GPUs (min 20GB VRAM each) + **RAID Storage**.

---

## ğŸ“¦ Installation (å®‰è£æ­¥é©Ÿ)

### 1. Clone Repository (ä¸‹è¼‰å°ˆæ¡ˆ)
    
    git clone [https://github.com/YourUsername/YaYan-AI.git](https://github.com/YourUsername/YaYan-AI.git)
    cd YaYan-AI
    mkdir -p models_cache input_audio output_text
    
### 2. Create Environment (å»ºç«‹ç’°å¢ƒ)
    
    conda create -n yayan_ai python=3.10 -y
    conda activate yayan_ai
    pip install -r requirements.txt
    
## â–¶ï¸ Usage (ä½¿ç”¨æ–¹æ³•)
### Option A: Running on Workstation (RTX 3090)
Uses Qwen-7B and Single GPU logic. é©ç”¨æ–¼å–®å¡é–‹ç™¼ç’°å¢ƒã€‚
    # Start Web UI
    python app.py

    # Batch Process (Input folder: ./input_audio)
    python auto_batch.py
    
Note: The first run will automatically download models (~15GB). Please wait. æ³¨æ„ï¼š é¦–æ¬¡åŸ·è¡Œå°‡è‡ªå‹•ä¸‹è¼‰æ¨¡å‹ï¼ˆç´„ 15GBï¼‰ï¼Œè«‹è€å¿ƒç­‰å¾…é€²åº¦æ¢è·‘å®Œã€‚
    
### Option B: Running on Server (Dual RTX 4000)
    # Start Web UI (Server Mode)
    python app_rtx4000.py

    # Batch Process (Input folder: /data/input_audio)
    python auto_batch_server.py
    
## ğŸ—ï¸ Technical Stack (æŠ€è¡“æ¶æ§‹)
    
* **Inference Engine:** PyTorch, Hugging Face Transformers

* **Quantization:** BitsAndBytes (NF4) for VRAM optimization

* **Audio Processing:** Librosa, SoundFile

* **Interface:** Gradio (WebUI)

* **Deployment:** Docker Ready (Server Edition)

## ğŸ“ License
    This project is open-source and available under the MIT License. 
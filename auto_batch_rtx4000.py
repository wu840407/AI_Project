import os
import sys
import torch
import time
import librosa
import soundfile as sf
import numpy as np
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig

# ==========================================
# 0. ç¡¬é«”èˆ‡è·¯å¾‘é…ç½® (Server ç‰ˆæ ¸å¿ƒè¨­å®š)
# ==========================================
# è¨­å®š Hugging Face å¿«å– (å»ºè­°æŒ‡å‘å¤§ç¡¬ç¢Ÿ)
os.environ["HF_HOME"] = "/data/models_cache" 

# è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºè³‡æ–™å¤¾ (ç›´æ¥æŒ‡å‘ RAID 10 å¤§ç¡¬ç¢Ÿ /data)
# æ‚¨éœ€è¦å…ˆåœ¨ /data å»ºç«‹é€™äº›è³‡æ–™å¤¾: mkdir -p /data/input_audio /data/output_text
INPUT_FOLDER = "/data/input_audio"
OUTPUT_FOLDER = "/data/output_text"

# æ”¯æ´çš„éŸ³æª”æ ¼å¼
SUPPORTED_EXTENSIONS = ('.mp3', '.wav', '.m4a', '.flac', '.ogg')

# é›¢ç·šæ¨¡å‹è·¯å¾‘ (å¦‚æœæœ‰çš„è©±ï¼Œå„ªå…ˆè®€å–é€™è£¡)
OFFLINE_MODEL_PATH_LLM = "/data/ai_models/Llama-3.1-8B-Instruct"
# å¦‚æœæ²’æœ‰é›¢ç·šæª”ï¼Œå°±ç”¨ç¶²è·¯ ID
ONLINE_MODEL_ID_LLM = "meta-llama/Meta-Llama-3.1-8B-Instruct"

# --- é›™å¡åˆ†é…é‚è¼¯ ---
if torch.cuda.device_count() >= 2:
    print(f"ğŸš€ [Server Mode] åµæ¸¬åˆ°é›™é¡¯å¡ï¼Œå•Ÿå‹•å¹³è¡ŒåŠ é€Ÿï¼")
    DEVICE_ASR = "cuda:0"  # GPU 0 è½
    DEVICE_LLM = "cuda:1"  # GPU 1 æƒ³
else:
    print(f"âš ï¸ [Single Mode] åƒ…åµæ¸¬åˆ°å–®å¡ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼ã€‚")
    DEVICE_ASR = "cuda:0"
    DEVICE_LLM = "cuda:0"

print(f"ğŸ¤ ASR Device: {DEVICE_ASR}")
print(f"ğŸ§  LLM Device: {DEVICE_LLM}")

# ==========================================
# 1. è¼‰å…¥æ¨¡å‹
# ==========================================

# --- A. è¼‰å…¥ Whisper (GPU 0) ---
print("â³ [1/2] æ­£åœ¨ GPU 0 è¼‰å…¥ Whisper-Large-v3...")
try:
    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model="openai/whisper-large-v3", # å¦‚æœæœ‰é›¢ç·šç‰ˆå¯æ”¹æˆè·¯å¾‘
        torch_dtype=torch.float16,
        device=DEVICE_ASR,
    )
except Exception as e:
    print(f"âŒ Whisper è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)

# --- B. è¼‰å…¥ Llama 3 (GPU 1) ---
# åˆ¤æ–·è¦ç”¨é›¢ç·šè·¯å¾‘é‚„æ˜¯ç·šä¸Š ID
if os.path.exists(OFFLINE_MODEL_PATH_LLM):
    print(f"â³ [2/2] æ­£åœ¨ GPU 1 è¼‰å…¥ Llama 3.1 (é›¢ç·šç‰ˆ: {OFFLINE_MODEL_PATH_LLM})...")
    model_source = OFFLINE_MODEL_PATH_LLM
else:
    print(f"â³ [2/2] æ­£åœ¨ GPU 1 è¼‰å…¥ Llama 3.1 (ç·šä¸Šä¸‹è¼‰: {ONLINE_MODEL_ID_LLM})...")
    model_source = ONLINE_MODEL_ID_LLM

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

try:
    tokenizer = AutoTokenizer.from_pretrained(model_source, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    llm_model = AutoModelForCausalLM.from_pretrained(
        model_source,
        quantization_config=bnb_config,
        device_map=DEVICE_LLM,  # <--- å¼·åˆ¶æŒ‡å®šåˆ° GPU 1
        trust_remote_code=True
    )
except Exception as e:
    print(f"âŒ LLM è¼‰å…¥å¤±æ•—: {e}")
    print("æç¤º: è‹¥ä½¿ç”¨ç·šä¸Š IDï¼Œè«‹ç¢ºèªå·²åŸ·è¡Œ `huggingface-cli login`")
    sys.exit(1)

# ==========================================
# 2. å®šç¾©è™•ç†é‚è¼¯
# ==========================================
def process_single_file(file_path, output_path):
    start_time = time.time()
    filename = os.path.basename(file_path)
    
    # --- A. å‰è™•ç†ï¼šPSTN å„ªåŒ– ---
    try:
        # å¼·åˆ¶è½‰ç‚º 16000Hz (Whisper åŸç”Ÿé »ç‡)
        y, sr = librosa.load(file_path, sr=16000)
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—: {filename}, éŒ¯èª¤: {e}")
        return

    # æ­£è¦åŒ– (Normalization)
    if np.max(np.abs(y)) > 0:
        y = y / np.max(np.abs(y))

    # --- B. Whisper è­˜åˆ¥ (GPU 0) ---
    try:
        asr_output = asr_pipe(
            {"raw": y, "sampling_rate": 16000}, 
            generate_kwargs={"task": "transcribe"},
            return_timestamps=True
        )
        raw_text = asr_output["text"]
    except Exception as e:
        print(f"âŒ Whisper è­˜åˆ¥å¤±æ•—: {filename}, éŒ¯èª¤: {e}")
        return

    # --- C. LLM åˆ†æ (GPU 1) ---
    # ä½¿ç”¨ Llama 3 çš„ Prompt æ ¼å¼
    system_prompt = """
    You are a linguistic expert specializing in Chinese dialects and Uyghur.
    The input text comes from phone recordings (ASR output) and may contain errors.
    
    Your task:
    1. [Identify]: Detect the language/dialect (e.g., Sichuan, Cantonese, Uyghur).
    2. [Translate]: Translate it into standard Traditional Chinese (æ­£é«”ä¸­æ–‡).
    3. [Correct]: Fix homophone errors based on context (e.g., "å››" vs "å").
    
    Output Format:
    [èªè¨€]: (Detected Language)
    [åŸæ–‡]: (Original ASR text)
    [è­¯æ–‡]: (Translated Text)
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Please process this text:\n{raw_text}"}
    ]

    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # æ³¨æ„ï¼šå°‡è¼¸å…¥æ¬ç§»åˆ° DEVICE_LLM (GPU 1)
    model_inputs = tokenizer([text_input], return_tensors="pt").to(DEVICE_LLM)

    with torch.no_grad():
        generated_ids = llm_model.generate(
            model_inputs.input_ids,
            max_new_tokens=1024,
            temperature=0.3,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    final_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    # --- D. å­˜æª” ---
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(final_output)

    duration = time.time() - start_time
    # ç°¡å–®é€²åº¦é¡¯ç¤º
    tqdm.write(f"âœ… å®Œæˆ: {filename} ({duration:.1f}s)")

# ==========================================
# 3. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists(INPUT_FOLDER):
        os.makedirs(INPUT_FOLDER)
        print(f"âš ï¸ å»ºç«‹è¼¸å…¥è³‡æ–™å¤¾: {INPUT_FOLDER}")
        print(f"ğŸ‘‰ è«‹å°‡éŸ³æª”æ”¾å…¥æ­¤è³‡æ–™å¤¾å¾Œé‡æ–°åŸ·è¡Œã€‚")
        sys.exit(0)
        
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    
    # æƒææª”æ¡ˆ
    all_files = [
        f for f in os.listdir(INPUT_FOLDER) 
        if f.lower().endswith(SUPPORTED_EXTENSIONS)
    ]
    
    if len(all_files) == 0:
        print(f"ğŸ“‚ {INPUT_FOLDER} æ˜¯ç©ºçš„ã€‚è«‹æ”¾å…¥éŸ³æª”ã€‚")
        sys.exit(0)

    print(f"\nğŸ“‚ ä¾†æº: {INPUT_FOLDER}")
    print(f"ğŸ“‚ ç›®çš„: {OUTPUT_FOLDER}")
    print(f"ğŸ“Š ç¸½è¨ˆ: {len(all_files)} å€‹æª”æ¡ˆ\n")
    
    # æ‰¹æ¬¡è™•ç†
    for filename in tqdm(all_files, desc="Processing"):
        input_path = os.path.join(INPUT_FOLDER, filename)
        output_filename = os.path.splitext(filename)[0] + "_report.txt"
        output_path = os.path.join(OUTPUT_FOLDER, output_filename)
        
        process_single_file(input_path, output_path)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
import os

# ==========================================
# 1. è¨­å®šæ¨¡å‹å„²å­˜è·¯å¾‘ (ä¸€å®šè¦åœ¨ import torch ä¹‹å‰)
# ==========================================
# é€™è¡Œæœƒè®“ Hugging Face æŠŠæ¨¡å‹ä¸‹è¼‰åˆ°æ‚¨å°ˆæ¡ˆåº•ä¸‹çš„ models_cache è³‡æ–™å¤¾
os.environ["HF_HOME"] = os.path.abspath("./models_cache")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from tqdm import tqdm
import time

# --- è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºè³‡æ–™å¤¾ ---
INPUT_FOLDER = "./input_audio"
OUTPUT_FOLDER = "./output_text"

# æ”¯æ´çš„éŸ³æª”æ ¼å¼
SUPPORTED_EXTENSIONS = ('.mp3', '.wav', '.m4a', '.flac', '.ogg')

# ==========================================
# 2. è¼‰å…¥æ¨¡å‹ (è‡ªå‹•ä¸‹è¼‰åˆ° D æ§½)
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»çµ±ï¼Œä½¿ç”¨è£ç½®: {device}")
print(f"ğŸ“‚ æ¨¡å‹å„²å­˜è·¯å¾‘: {os.environ['HF_HOME']}")

print("â³ [1/2] æ­£åœ¨è¼‰å…¥ Whisper-Large-v3 (è² è²¬è½)...")
# Whisper æœƒè‡ªå‹•ä¸‹è¼‰åˆ° models_cache
asr_pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3",
    torch_dtype=torch.float16,
    device=device,
)

print("â³ [2/2] æ­£åœ¨è¼‰å…¥ Qwen-2.5-7B (è² è²¬ç¿»è­¯)...")
llm_model_id = "Qwen/Qwen2.5-7B-Instruct"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
tokenizer = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# ==========================================
# 3. å®šç¾©è™•ç†é‚è¼¯
# ==========================================
def process_single_file(file_path, output_path):
    start_time = time.time()
    filename = os.path.basename(file_path)
    
    # --- A. Whisper è‡ªå‹•è­˜åˆ¥ ---
    # æˆ‘å€‘ä¸æŒ‡å®š languageï¼Œè®“ Whisper è‡ªå·±çŒœ (å®ƒæ”¯æ´ auto detect)
    # return_timestamps=True è®“å®ƒè™•ç†é•·éŸ³æª”æ›´ç©©å®š
    try:
        asr_output = asr_pipe(
            file_path, 
            generate_kwargs={"task": "transcribe"}, # transcribe = è½‰éŒ„åŸæ–‡
            return_timestamps=True
        )
        raw_text = asr_output["text"]
    except Exception as e:
        print(f"âŒ Whisper è­˜åˆ¥å¤±æ•—: {filename}, éŒ¯èª¤: {e}")
        return

    # --- B. LLM ç¿»è­¯èˆ‡æ–¹è¨€è­˜åˆ¥ ---
    # é€™è£¡çš„ Prompt æ˜¯é—œéµï¼Œæˆ‘å€‘è®“ Qwen è‡ªå·±å»åˆ¤æ–·åŸæ–‡æ˜¯å“ªç¨®æ–¹è¨€
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ç²¾é€šæ¼¢èªæ–¹è¨€ï¼ˆå››å·è©±ã€ä¸Šæµ·è©±ã€å»£æ±è©±ã€é–©å—èªï¼‰ä»¥åŠç¶­å¾çˆ¾èªçš„èªè¨€å­¸å®¶ã€‚
    
    ä½¿ç”¨è€…çš„è¼¸å…¥æ˜¯ä¸€æ®µèªéŸ³è­˜åˆ¥ï¼ˆASRï¼‰å¾Œçš„æ–‡å­—ã€‚
    ä½ çš„ä»»å‹™æ˜¯ï¼š
    1. ã€åˆ¤æ–·èªè¨€ã€‘ï¼šåˆ†æé€™æ®µæ–‡å­—å±¬æ–¼å“ªç¨®èªè¨€æˆ–æ–¹è¨€ã€‚
    2. ã€ç¿»è­¯ã€‘ï¼šå°‡å…¶æº–ç¢ºç¿»è­¯ç‚ºã€Œæ¨™æº–æ­£é«”ä¸­æ–‡ã€ã€‚
    3. ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼šè«‹åš´æ ¼ä¾ç…§ä¸‹æ–¹æ ¼å¼è¼¸å‡ºï¼Œä¸è¦æœ‰å¤šé¤˜å»¢è©±ã€‚
    
    æ ¼å¼ç¯„ä¾‹ï¼š
    [èªè¨€]: å››å·è©±
    [åŸæ–‡]: (é€™è£¡æ”¾åŸæœ¬è­˜åˆ¥çš„æ–‡å­—)
    [è­¯æ–‡]: (é€™è£¡æ”¾æ­£é«”ä¸­æ–‡ç¿»è­¯)
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è«‹è™•ç†é€™æ®µè­˜åˆ¥æ–‡å­—ï¼š\n{raw_text}"}
    ]

    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text_input], return_tensors="pt").to(device)

    # ç”¢ç”Ÿç¿»è­¯
    with torch.no_grad():
        generated_ids = llm_model.generate(
            model_inputs.input_ids,
            max_new_tokens=1024,
            temperature=0.3 # ç¿»è­¯éœ€è¦æº–ç¢ºåº¦ï¼Œæº«åº¦èª¿ä½
        )
    
    final_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    # ç§»é™¤ Prompt éƒ¨åˆ†ï¼Œåªä¿ç•™ AI å›ç­” (Qwenæœ‰æ™‚æœƒåŒ…å« promptï¼Œè¦–ç‰ˆæœ¬è€Œå®šï¼Œé€šå¸¸ skip_special_tokens å°±å¤ äº†ï¼Œé€™è£¡åšå­—ä¸²è™•ç†ä¿éšª)
    # é€™è£¡å‡è¨­ Qwen ç›´æ¥è¼¸å‡ºå›ç­”ã€‚è‹¥æœ‰åŒ…å« inputï¼Œé€šå¸¸åœ¨ tokenizer decode æ™‚æœƒè™•ç†ï¼Œæˆ–æ˜¯ç”¨ output_ids[len(input_ids):] åˆ‡å‰²
    # ç‚ºäº†ç¨‹å¼ç¢¼ç°¡æ½”ï¼Œé€™è£¡ä½¿ç”¨ç°¡å–®çš„åˆ‡å‰²æ³•ï¼ˆå¦‚æœæ¨¡å‹è¼¸å‡ºäº† promptï¼‰
    if "è«‹è™•ç†é€™æ®µè­˜åˆ¥æ–‡å­—" in final_output:
         final_output = final_output.split("è«‹è™•ç†é€™æ®µè­˜åˆ¥æ–‡å­—ï¼š")[-1].strip()

    # --- C. å­˜æª” ---
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(final_output)

    duration = time.time() - start_time
    print(f"âœ… å®Œæˆ: {filename} (è€—æ™‚ {duration:.2f}ç§’)")

# ==========================================
# 4. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    
    # æƒææª”æ¡ˆ
    all_files = [
        f for f in os.listdir(INPUT_FOLDER) 
        if f.lower().endswith(SUPPORTED_EXTENSIONS)
    ]
    
    print(f"\nğŸ“‚ åœ¨ {INPUT_FOLDER} æ‰¾åˆ° {len(all_files)} å€‹éŸ³æª”ï¼Œæº–å‚™é–‹å§‹è™•ç†...\n")
    
    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    for filename in tqdm(all_files, desc="ç¸½é€²åº¦"):
        input_path = os.path.join(INPUT_FOLDER, filename)
        output_filename = os.path.splitext(filename)[0] + "_translated.txt"
        output_path = os.path.join(OUTPUT_FOLDER, output_filename)
        
        process_single_file(input_path, output_path)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è«‹æŸ¥çœ‹ {OUTPUT_FOLDER} è³‡æ–™å¤¾ã€‚")
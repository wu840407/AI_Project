import os

# ==========================================
# 1. è¨­å®šæ¨¡å‹å„²å­˜è·¯å¾‘ (ä¸€å®šè¦åœ¨ import torch ä¹‹å‰)
# ==========================================
# é€™è¡Œæœƒè®“ Hugging Face æŠŠæ¨¡å‹ä¸‹è¼‰åˆ°æ‚¨å°ˆæ¡ˆåº•ä¸‹çš„ models_cache è³‡æ–™å¤¾
os.environ["HF_HOME"] = os.path.abspath("./models_cache")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from tqdm import tqdm
import time
import librosa # ç¢ºä¿é–‹é ­æœ‰ import é€™å€‹åº« (åŸæœ¬ç’°å¢ƒæ‡‰è©²å·²å®‰è£)
import soundfile as sf
import numpy as np

# --- è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºè³‡æ–™å¤¾ ---
INPUT_FOLDER = "./input_audio"
OUTPUT_FOLDER = "./output_text"

# æ”¯æ´çš„éŸ³æª”æ ¼å¼
SUPPORTED_EXTENSIONS = ('.mp3', '.wav', '.m4a', '.flac', '.ogg')

# ==========================================
# 2. è¼‰å…¥æ¨¡å‹ (è‡ªå‹•ä¸‹è¼‰åˆ° D æ§½)
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»çµ±ï¼Œä½¿ç”¨è£ç½®: {device}")
print(f"ğŸ“‚ æ¨¡å‹å„²å­˜è·¯å¾‘: {os.environ['HF_HOME']}")

print("â³ [1/2] æ­£åœ¨è¼‰å…¥ Whisper-Large-v3 (è² è²¬è½)...")
# Whisper æœƒè‡ªå‹•ä¸‹è¼‰åˆ° models_cache
asr_pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3",
    torch_dtype=torch.float16,
    device=device,
)

print("â³ [2/2] æ­£åœ¨è¼‰å…¥ Qwen-2.5-7B (è² è²¬ç¿»è­¯)...")
llm_model_id = "Qwen/Qwen2.5-7B-Instruct"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
tokenizer = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# ==========================================
# 3. å®šç¾©è™•ç†é‚è¼¯
# ==========================================
def process_single_file(file_path, output_path):
    start_time = time.time()
    filename = os.path.basename(file_path)
    
    print(f"ğŸ”„ æ­£åœ¨å„ªåŒ–é›»è©±éŸ³è¨Š: {filename} ...")

    # --- A. å‰è™•ç†ï¼šé‡å° PSTN/VoIP å„ªåŒ– ---
    # 1. ä½¿ç”¨ librosa è®€å–ï¼Œä¸¦å¼·åˆ¶è½‰ç‚º 16000Hz (Whisper çš„åŸç”Ÿé »ç‡)
    # y æ˜¯éŸ³è¨Šæ•¸æ“š, sr æ˜¯å–æ¨£ç‡
    try:
        y, sr = librosa.load(file_path, sr=16000)
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—: {filename}, éŒ¯èª¤: {e}")
        return

    # 2. ç°¡å–®çš„æ­£è¦åŒ– (Normalization) - è®“å°è²çš„é›»è©±éŒ„éŸ³è®Šå¤§è²
    if np.max(np.abs(y)) > 0:
        y = y / np.max(np.abs(y))

    # 3. ç‚ºäº†è®“ pipeline è®€å–ï¼Œæˆ‘å€‘éœ€è¦å‚³é numpy array æˆ–è€…æš«å­˜æª”
    # é€™è£¡æˆ‘å€‘ç›´æ¥æŠŠå„ªåŒ–å¾Œçš„è²éŸ³å‚³çµ¦ Whisperï¼Œä¸å­˜æš«å­˜æª”ä»¥æ±‚é€Ÿåº¦
    
    # --- B. Whisper è‡ªå‹•è­˜åˆ¥ ---
    try:
        # pipeline å¯ä»¥ç›´æ¥åƒ numpy array (éœ€è¦çµ¦ sampling_rate)
        asr_output = asr_pipe(
            {"raw": y, "sampling_rate": 16000}, 
            generate_kwargs={"task": "transcribe"},
            return_timestamps=True
        )
        raw_text = asr_output["text"]
    except Exception as e:
        print(f"âŒ Whisper è­˜åˆ¥å¤±æ•—: {filename}, éŒ¯èª¤: {e}")
        return

    # --- C. LLM ç¿»è­¯èˆ‡æ–¹è¨€è­˜åˆ¥ (é€™æ®µä¸ç”¨æ”¹ï¼Œç¶­æŒåŸæ¨£) ---
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ç²¾é€šæ¼¢èªæ–¹è¨€ï¼ˆå››å·è©±ã€ä¸Šæµ·è©±ã€å»£æ±è©±ã€é–©å—èªï¼‰ä»¥åŠç¶­å¾çˆ¾èªçš„èªè¨€å­¸å®¶ã€‚
    ä½¿ç”¨è€…çš„è¼¸å…¥æ˜¯ä¸€æ®µèªéŸ³è­˜åˆ¥ï¼ˆASRï¼‰å¾Œçš„æ–‡å­—ï¼Œä¾†æºæ˜¯é›»è©±éŒ„éŸ³ï¼ˆå¯èƒ½åŒ…å«é›œè¨Šæˆ–æ¨¡ç³Šç™¼éŸ³ï¼‰ã€‚
    
    ä½ çš„ä»»å‹™æ˜¯ï¼š
    1. ã€åˆ¤æ–·èªè¨€ã€‘ï¼šåˆ†æé€™æ®µæ–‡å­—å±¬æ–¼å“ªç¨®èªè¨€æˆ–æ–¹è¨€ã€‚
    2. ã€ç¿»è­¯ã€‘ï¼šå°‡å…¶æº–ç¢ºç¿»è­¯ç‚ºã€Œæ¨™æº–æ­£é«”ä¸­æ–‡ã€ã€‚
    3. ã€ä¿®æ­£ã€‘ï¼šé›»è©±éŒ„éŸ³å¸¸å°‡ã€Œå››ã€è½æˆã€Œåã€ã€ã€Œç™¼ã€è½æˆã€ŒèŠ±ã€ï¼Œè«‹æ ¹æ“šèªå¢ƒä¿®æ­£ã€‚
    
    æ ¼å¼ç¯„ä¾‹ï¼š
    [èªè¨€]: å››å·è©±
    [åŸæ–‡]: (é€™è£¡æ”¾åŸæœ¬è­˜åˆ¥çš„æ–‡å­—)
    [è­¯æ–‡]: (é€™è£¡æ”¾æ­£é«”ä¸­æ–‡ç¿»è­¯)
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è«‹è™•ç†é€™æ®µé›»è©±è­˜åˆ¥æ–‡å­—ï¼š\n{raw_text}"}
    ]

    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text_input], return_tensors="pt").to(device)

    with torch.no_grad():
        generated_ids = llm_model.generate(
            model_inputs.input_ids,
            max_new_tokens=1024,
            temperature=0.3
        )
    
    final_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    if "è«‹è™•ç†é€™æ®µé›»è©±è­˜åˆ¥æ–‡å­—" in final_output:
         final_output = final_output.split("è«‹è™•ç†é€™æ®µé›»è©±è­˜åˆ¥æ–‡å­—ï¼š")[-1].strip()

    # --- D. å­˜æª” ---
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(final_output)

    duration = time.time() - start_time
    print(f"âœ… å®Œæˆ: {filename} (è€—æ™‚ {duration:.2f}ç§’)")

# ==========================================
# 4. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    
    # æƒææª”æ¡ˆ
    all_files = [
        f for f in os.listdir(INPUT_FOLDER) 
        if f.lower().endswith(SUPPORTED_EXTENSIONS)
    ]
    
    print(f"\nğŸ“‚ åœ¨ {INPUT_FOLDER} æ‰¾åˆ° {len(all_files)} å€‹éŸ³æª”ï¼Œæº–å‚™é–‹å§‹è™•ç†...\n")
    
    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    for filename in tqdm(all_files, desc="ç¸½é€²åº¦"):
        input_path = os.path.join(INPUT_FOLDER, filename)
        output_filename = os.path.splitext(filename)[0] + "_translated.txt"
        output_path = os.path.join(OUTPUT_FOLDER, output_filename)
        
        process_single_file(input_path, output_path)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è«‹æŸ¥çœ‹ {OUTPUT_FOLDER} è³‡æ–™å¤¾ã€‚")